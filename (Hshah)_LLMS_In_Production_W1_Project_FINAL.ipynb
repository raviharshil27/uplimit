{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raviharshil27/uplimit/blob/main/(Hshah)_LLMS_In_Production_W1_Project_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataAlchemy Labs\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1jLQB9IRtp4cHnIxRSebSVcclKxNfdHCf\" />\n",
        "\n",
        "Welcome to the week 1 project for LLMs in Production. In this weeks project you are a founding Machine Learning Engineer on the team you and your team have narrowed down a life-changing product. You will be building and launching your groundbreaking SaaS product which is quite similar to a product already out in the market, [AI2sql](https://www.ai2sql.io/)."
      ],
      "metadata": {
        "id": "j8srXs0p12CE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![AI2sql Example](https://drive.google.com/uc?id=1mWOQs6OZmuCLui-auLosXsXGhwl2mbVg)\n",
        "\n",
        "You have done a significant amount of market research and are confident that this product will gather a cult-like following with a killer feature of being able to take any natural language query and output SQL code based on the query.\n",
        "\n",
        "Potential users of your application are:\n",
        "- **Project Managers**: They can use our tool to quickly derive important metrics for presentations or reports. Instead of relying on technical teams, they can directly query the database using natural language.\n",
        "- **Data Scientists**: Often dealing with complex data queries, data scientists can use our tool to streamline and debug intricate SQL statements, saving time and reducing the potential for error.\n",
        "- **Business Analysts and Non-Tech Professionals**: Anyone without deep SQL knowledge but needing to interact with the database can benefit immensely. They can easily convert their data needs into SQL queries without delving into the complexities of SQL syntax.\n",
        "\n",
        "There are many more features that you can work on in the future such as:\n",
        "1. NoSQL Code Generation\n",
        "2. SQL Syntax Checking\n",
        "3. Explaining SQL Code\n",
        "4. Optimizing SQL Code\n",
        "5. Formatting SQL Code\n",
        "\n",
        "But since this is an MVP and you want to laucnh as quickly as possible we will be just focussing on the use case of SQL Code Generation. In particular we will be supporting the use of **SQL** initially as its the most popular database out there in use currently based on the recent [StackOverflow Developer Survey 2023](https://survey.stackoverflow.co/2023/#section-most-popular-technologies-databases).\n",
        "\n",
        "\n",
        "\n",
        "Our final product will be a simple web application with a basic Q/A system that allows users to ask query about SQL and then get back SQL code that they can use for their application, think of it as a super basic version of the ChatGPT from OpenAI that I am sure everyone is quite familiar with.\n",
        "\n",
        "![ChatGPT OpenAI Example](https://drive.google.com/uc?id=1AGVZoxtWvBF6KLX0mkvpnU7mMEgaSraa)\n",
        "\n",
        "In the Week 1 Project we will be solidfying what we learnt throughout the week by building out this core functionality for the SQL Code Generation part of our application. Just like all Data Science projects we start off in a notebook environment to prototype things before having to bring them to production.\n",
        "\n",
        "Just like most LLM startups we will be leveraging OpenAI and their ChatGPT API initially for our product, in particular we will be using **gpt-4o-mini** which offers a good enough LLM that is quite fast!\n",
        "\n",
        "The main parts that we will be covering in the project are:\n",
        "1. [Using LLMs to Generate SQL Code](#scrollTo=HcGIbmPiM4o3)\n",
        "2. [Evaluating LLMs on SQL Code Generation](#scrollTo=pWz1btwOM7zr)\n",
        "3. [Validating LLM Outputs using Guardrails](#scrollTo=rPmPTQssNDUn)\n",
        "4. [Extra Credit: Optional Tasks](#scrollTo=UL-XI8PiNU1z&line=1&uniqifier=1)\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/10y3EFKr4S8TsiSp00y8_LTD-2UU8nCGS?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6lSc2PMKFPyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up OpenAI API Key"
      ],
      "metadata": {
        "id": "9MduGWtPIPTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "  padding: 10px;\n",
        "  border-radius: 5px;\n",
        "  background-color: #ffcccc;\n",
        "  border-left: 6px solid #ff0000;\n",
        "  margin-bottom: 20px;\">\n",
        "  \n",
        "  <strong>⚠️ Important Notice:</strong>\n",
        "  <p>Do not share or use this API Key outside of the context of the notebook exercises.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "q1v3j1VcIWFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uplimit has provisioned an OpenAI API Key for your projects. Please add this API Key to this assignment by clicking on the Security Key icon on the left hand tab of the Google Colab notebook and then add a new parameter value called `OPENAI_API_KEY`.\n",
        "\n",
        "\n",
        " Here you can provide the API key that you copied and this will not be part of your Google Colab account. You can also enable the toggle Notebook access - this will allow your notebook to have access to this API key.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1SfE-nNOQ3DfZpJN6mAnxuqVsMv-F1_Yp\" />\n",
        "\n",
        "**NOTE:** We are hardcoding an API key for [Guardrails.ai](https://www.guardrailsai.com/) which is used for downloading different types of LLM Guardrails, access is free regardless and we hardcode it as a convenience for all the students."
      ],
      "metadata": {
        "id": "RzMCOyXfIZCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the API Key has been setup, run the following code:"
      ],
      "metadata": {
        "id": "nMMB_lzHIdpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai guardrails-ai==0.5.0 deepeval langchain langchain-openai"
      ],
      "metadata": {
        "id": "vcOQFQC7jUU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f31c344-b337-42f8-8b61-3b047eb11637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n",
            "Collecting guardrails-ai==0.5.0\n",
            "  Downloading guardrails_ai-0.5.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting deepeval\n",
            "  Downloading deepeval-1.5.0-py3-none-any.whl.metadata (977 bytes)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting coloredlogs<16.0.0,>=15.0.1 (from guardrails-ai==0.5.0)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting faker<26.0.0,>=25.2.0 (from guardrails-ai==0.5.0)\n",
            "  Downloading Faker-25.9.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting griffe<0.37.0,>=0.36.9 (from guardrails-ai==0.5.0)\n",
            "  Downloading griffe-0.36.9-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting guardrails-api-client>=0.3.8 (from guardrails-ai==0.5.0)\n",
            "  Downloading guardrails_api_client-0.3.13-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from guardrails-ai==0.5.0)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (4.23.0)\n",
            "Collecting langchain-core<0.3,>=0.1 (from guardrails-ai==0.5.0)\n",
            "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting litellm<2.0.0,>=1.37.14 (from guardrails-ai==0.5.0)\n",
            "  Downloading litellm-1.52.3-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting lxml<5.0.0,>=4.9.3 (from guardrails-ai==0.5.0)\n",
            "  Downloading lxml-4.9.4-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (3.8.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 (from guardrails-ai==0.5.0)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.24.0 (from guardrails-ai==0.5.0)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.24.0 (from guardrails-ai==0.5.0)\n",
            "  Downloading opentelemetry_sdk-1.28.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pip>=22 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (24.1.2)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2.9.2)\n",
            "Collecting pydash<8.0.0,>=7.0.6 (from guardrails-ai==0.5.0)\n",
            "  Downloading pydash-7.0.7-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2.9.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2.8.2)\n",
            "Collecting regex<2024.0.0,>=2023.10.3 (from guardrails-ai==0.5.0)\n",
            "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.6.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (13.9.3)\n",
            "Collecting rstr<4.0.0,>=3.2.2 (from guardrails-ai==0.5.0)\n",
            "  Downloading rstr-3.2.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: tenacity>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (9.0.0)\n",
            "Collecting tiktoken>=0.5.1 (from guardrails-ai==0.5.0)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting typer<0.10.0,>=0.9.0 (from typer[all]<0.10.0,>=0.9.0->guardrails-ai==0.5.0)\n",
            "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from deepeval) (7.4.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from deepeval) (3.20.3)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.10/dist-packages (from deepeval) (2.17.0)\n",
            "Collecting pytest-repeat (from deepeval)\n",
            "  Downloading pytest_repeat-0.9.3-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pytest-xdist (from deepeval)\n",
            "  Downloading pytest_xdist-3.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting portalocker (from deepeval)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting ragas (from deepeval)\n",
            "  Downloading ragas-0.2.4-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting docx2txt~=0.8 (from deepeval)\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata>=6.0.2 in /usr/local/lib/python3.10/dist-packages (from deepeval) (8.5.0)\n",
            "Collecting tenacity>=8.1.0 (from guardrails-ai==0.5.0)\n",
            "  Downloading tenacity-8.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting opentelemetry-api~=1.24.0 (from deepeval)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting opentelemetry-sdk<2.0.0,>=1.24.0 (from guardrails-ai==0.5.0)\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 (from guardrails-ai==0.5.0)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting grpcio~=1.63.0 (from deepeval)\n",
            "  Downloading grpcio-1.63.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.6-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.5-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs<16.0.0,>=15.0.1->guardrails-ai==0.5.0)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting colorama>=0.4 (from griffe<0.37.0,>=0.36.9->guardrails-ai==0.5.0)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-api-client>=0.3.8->guardrails-ai==0.5.0) (75.1.0)\n",
            "Collecting urllib3<2.1.0,>=1.25.3 (from guardrails-api-client>=0.3.8->guardrails-ai==0.5.0)\n",
            "  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.0.2->deepeval) (3.20.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (0.20.0)\n",
            "Collecting fqdn (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (3.0.0)\n",
            "Collecting rfc3339-validator (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3987 (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0)\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting uri-template (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (24.8.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1->guardrails-ai==0.5.0) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1->guardrails-ai==0.5.0) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (8.1.7)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (3.1.4)\n",
            "Collecting openai\n",
            "  Downloading openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting python-dotenv>=0.2.0 (from litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (0.19.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->guardrails-ai==0.5.0) (1.4.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api~=1.24.0->deepeval) (1.2.14)\n",
            "Collecting importlib-metadata>=6.0.2 (from deepeval)\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai==0.5.0) (1.65.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai==0.5.0)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai==0.5.0)\n",
            "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-exporter-otlp-proto-http to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.24.0 (from guardrails-ai==0.5.0)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.24.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-sdk<2.0.0,>=1.24.0->guardrails-ai==0.5.0)\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=2.0.0->guardrails-ai==0.5.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=2.0.0->guardrails-ai==0.5.0) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.2->guardrails-ai==0.5.0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->guardrails-ai==0.5.0) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.6.0->guardrails-ai==0.5.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.6.0->guardrails-ai==0.5.0) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<0.10.0,>=0.9.0->guardrails-ai==0.5.0) (1.5.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval) (1.5.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval) (2.0.2)\n",
            "Collecting execnet>=2.1 (from pytest-xdist->deepeval)\n",
            "  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting datasets (from ragas->deepeval)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting langchain-community (from ragas->deepeval)\n",
            "  Downloading langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (1.6.0)\n",
            "Collecting appdirs (from ragas->deepeval)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pysbd>=0.3.4 (from ragas->deepeval)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api~=1.24.0->deepeval) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->guardrails-ai==0.5.0) (0.1.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas->deepeval)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (2.2.2)\n",
            "Collecting xxhash (from datasets->ragas->deepeval)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->ragas->deepeval)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas->deepeval)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (0.24.7)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community->ragas->deepeval)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community->ragas->deepeval)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community (from ragas->deepeval)\n",
            "  Downloading langchain_community-0.3.4-py3-none-any.whl.metadata (2.9 kB)\n",
            "  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading langchain_community-0.2.18-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0)\n",
            "  Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas->deepeval) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas->deepeval) (2024.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading guardrails_ai-0.5.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepeval-1.5.0-py3-none-any.whl (460 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.6/460.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.2.17-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.1.25-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Faker-25.9.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading griffe-0.36.9-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.63.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading guardrails_api_client-0.3.13-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.2/108.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Downloading litellm-1.52.3-py3-none-any.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.54.3-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-4.9.4-cp310-cp310-manylinux_2_28_x86_64.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.24.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Downloading pydash-7.0.7-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.3/110.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rstr-3.2.2-py3-none-any.whl (10 kB)\n",
            "Downloading tenacity-8.4.2-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading pytest_repeat-0.9.3-py3-none-any.whl (4.2 kB)\n",
            "Downloading pytest_xdist-3.6.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ragas-0.2.4-py3-none-any.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.9/151.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading execnet-2.1.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading langchain_community-0.2.18-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=e77803b6fb9f77ee3132ea522d91fa07398cbb0c28ea4803b0fa256015bc8d06\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: rfc3987, docx2txt, appdirs, xxhash, urllib3, uri-template, types-python-dateutil, typer, tenacity, rstr, rfc3339-validator, regex, python-dotenv, pysbd, pydash, portalocker, opentelemetry-semantic-conventions, opentelemetry-proto, mypy-extensions, marshmallow, lxml, jsonref, importlib-metadata, humanfriendly, grpcio, fsspec, fqdn, execnet, dill, colorama, typing-inspect, pytest-xdist, pytest-repeat, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, multiprocess, griffe, faker, coloredlogs, arrow, tiktoken, opentelemetry-sdk, openai, isoduration, guardrails-api-client, dataclasses-json, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, litellm, langchain-core, datasets, langchain-text-splitters, langchain-openai, guardrails-ai, langchain, langchain-community, ragas, deepeval\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.3\n",
            "    Uninstalling urllib3-2.2.3:\n",
            "      Successfully uninstalled urllib3-2.2.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.12.5\n",
            "    Uninstalling typer-0.12.5:\n",
            "      Successfully uninstalled typer-0.12.5\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.9.11\n",
            "    Uninstalling regex-2024.9.11:\n",
            "      Successfully uninstalled regex-2024.9.11\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.3.0\n",
            "    Uninstalling lxml-5.3.0:\n",
            "      Successfully uninstalled lxml-5.3.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.5.0\n",
            "    Uninstalling importlib_metadata-8.5.0:\n",
            "      Successfully uninstalled importlib_metadata-8.5.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.64.1\n",
            "    Uninstalling grpcio-1.64.1:\n",
            "      Successfully uninstalled grpcio-1.64.1\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.16.0\n",
            "    Uninstalling opentelemetry-api-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.16.0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.16.0\n",
            "    Uninstalling opentelemetry-sdk-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.52.2\n",
            "    Uninstalling openai-1.52.2:\n",
            "      Successfully uninstalled openai-1.52.2\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.13\n",
            "    Uninstalling langchain-core-0.3.13:\n",
            "      Successfully uninstalled langchain-core-0.3.13\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.0\n",
            "    Uninstalling langchain-text-splitters-0.3.0:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.0\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.4\n",
            "    Uninstalling langchain-0.3.4:\n",
            "      Successfully uninstalled langchain-0.3.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 arrow-1.3.0 colorama-0.4.6 coloredlogs-15.0.1 dataclasses-json-0.6.7 datasets-3.1.0 deepeval-1.5.0 dill-0.3.8 docx2txt-0.8 execnet-2.1.1 faker-25.9.2 fqdn-1.5.1 fsspec-2024.9.0 griffe-0.36.9 grpcio-1.63.2 guardrails-ai-0.5.0 guardrails-api-client-0.3.13 humanfriendly-10.0 importlib-metadata-7.0.0 isoduration-20.11.0 jsonref-1.1.0 langchain-0.2.17 langchain-community-0.2.18 langchain-core-0.2.43 langchain-openai-0.1.25 langchain-text-splitters-0.2.4 litellm-1.52.3 lxml-4.9.4 marshmallow-3.23.1 multiprocess-0.70.16 mypy-extensions-1.0.0 openai-1.54.3 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-exporter-otlp-proto-http-1.24.0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 portalocker-2.10.1 pydash-7.0.7 pysbd-0.3.4 pytest-repeat-0.9.3 pytest-xdist-3.6.1 python-dotenv-1.0.1 ragas-0.2.4 regex-2023.12.25 rfc3339-validator-0.1.4 rfc3987-1.3.8 rstr-3.2.2 tenacity-8.4.2 tiktoken-0.8.0 typer-0.9.4 types-python-dateutil-2.9.0.20241003 typing-inspect-0.9.0 uri-template-1.3.0 urllib3-2.0.7 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"GUARDRAILS_TOKEN\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDEwNzI3MDE2NDYwNDM4MTg2OTU4OSIsImFwaUtleUlkIjoiYjAzNDVkNGEtZDhjNy00OThmLWIwZGYtZWI3ZTY0MzMwNTJkIiwiaWF0IjoxNzMwMTQ2ODc0LCJleHAiOjE3Mzc5MjI4NzR9.1J84O3pT_KlWTkGjV4zfBMChqYow868A-XTsePwiZ_Q\""
      ],
      "metadata": {
        "id": "OFBxDLxcIViM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Using LLMs to Generate SQL Code\n",
        "\n",
        "## SQL Code Generation\n",
        "\n",
        "SQL is one of the most powerful programming languages out there and is the main way in which we communicate with databases. Without it backend engineers would be unable to store and structure data to be easily queried to return to a frontend application and data scientists would be unable to track key metrics about their experiments over time.\n",
        "\n",
        "SQL is the language used for interacting with databases, while SQL is a specific database system that understands and uses SQL for database operations. Each database system, including SQL, implements SQL with some variations and adds its own proprietary extensions to the standard SQL language.\n",
        "\n",
        "Here is an example SQL statement:\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "    u.name,\n",
        "    u.email,\n",
        "    COUNT(o.order_id) AS total_orders,\n",
        "    SUM(o.amount) AS total_amount_spent\n",
        "FROM\n",
        "    users u\n",
        "JOIN\n",
        "    orders o ON u.user_id = o.user_id\n",
        "WHERE\n",
        "    o.order_date >= (NOW() - INTERVAL '1 year')\n",
        "GROUP BY\n",
        "    u.user_id\n",
        "HAVING\n",
        "    COUNT(o.order_id) > 5\n",
        "ORDER BY\n",
        "    total_amount_spent DESC;\n",
        "```\n",
        "\n",
        "LLM's such as ChatGPT from OpenAI are trained on all kinds of data across the internet and in particular they are trained on code across many different programming languages.\n",
        "\n",
        "\n",
        "![Text2SQL](https://drive.google.com/uc?id=17IDU11L_JdPIfMWvDa6li_5oOyGgWriX)\n",
        "\n",
        "What we will be performing is asking the LLM a Question related to a SQL query in order to create SQL code that we can then run.\n",
        "\n",
        "\n",
        "During OpenAI DevDay, there was a fantastic breakout section about **\"[A Survey of Techniques for Maximizing LLM Performance](https://youtu.be/ahnGLM-RC1Y?si=rifoSUxFgvgliFtT)\"** which we recommend all of you go through. In that talk, the speakers highlighted how we should always be starting off with Prompt Engineering when building LLM applications. Once we understand what our LLM application lacks can we only try more complex techniques such as RAG or Fine-tuning or both. With that spirit in mind let's try out some basic Prompt Engineering using ChatGPT.\n",
        "\n",
        "![Optimization Flow from OpenAI](https://drive.google.com/uc?id=1kcguLT8KYBmDypGB0fVyeT8YfrCiiMx_)"
      ],
      "metadata": {
        "id": "HcGIbmPiM4o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import LLMResult, HumanMessage, Generation\n",
        "\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=MODEL_NAME,\n",
        "    temperature=0.0\n",
        ")"
      ],
      "metadata": {
        "id": "82LGwE57Zc4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Create a Baseline Zero-Shot Prompt\n",
        "\n",
        "Create a Zero-Shot Prompt that we can use in this situation, we have provided you with a simple prompt to try out. A **Zero-Shot prompt** is where we are asking the model to perform a task it has never seen explicty during training without any additional examples.\n",
        "\n",
        "When working on the prompt or improving the prompt remember a couple of things:\n",
        "\n",
        "1. Make sure to use clear instructions\n",
        "2. Try splitting tasks into simpler subtasks\n",
        "3. Give the model time to “think”\n",
        "\n",
        "![How to Prompt Engineer](https://drive.google.com/uc?id=1iSZ6XvD5eMiRSQBUT0DFEDA-1qc0qibv)\n"
      ],
      "metadata": {
        "id": "RE5HXufjHF8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Add your prompt here, make sure to use\n",
        "# {query} to inject a query into the prompt.\n",
        "\n",
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"\n",
        "You will be given a prompt which you will have to translate to SQL.\n",
        "Here is the query: {query}\n",
        "\"\"\"\n",
        "\n",
        "# Try your own query\n",
        "SAMPLE_QUERY = \"Select the name of the employee who has the highest salary\"\n",
        "\n",
        "prompt = ZERO_SHOT_PROMPT_TEMPLATE.format(\n",
        "    query=SAMPLE_QUERY\n",
        ")\n",
        "\n",
        "result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "\n",
        "# # Do try to investigate the generations from LangChain to understand what is generated\n",
        "print(result.generations[0][0].text)"
      ],
      "metadata": {
        "id": "xcLCb77HbZ1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f7d2644-4995-45ca-d0ef-6c02112648f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To select the name of the employee who has the highest salary, you can use the following SQL query:\n",
            "\n",
            "```sql\n",
            "SELECT name\n",
            "FROM employees\n",
            "WHERE salary = (SELECT MAX(salary) FROM employees);\n",
            "```\n",
            "\n",
            "In this query, we assume that the table containing employee information is named `employees`, and it has columns `name` and `salary`. The subquery `(SELECT MAX(salary) FROM employees)` retrieves the highest salary, and the outer query selects the name of the employee(s) who have that salary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job! We have generated our very first SQL code using an LLM!\n",
        "\n",
        "An improvement we can perform on this is using a **Few-Shot prompt** where we include a couple of examples for the LLM to understand what we are trying to do. Doing so should result in better results as the LLM has some reference examples about what we are trying to do.\n",
        "\n",
        "**BUT** remember *we need to evaluate our LLM* prompt before we can even think about experimenting on it otherwise how would we even know if this new prompt is even better in the first place?"
      ],
      "metadata": {
        "id": "ys1WKl5JdIoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Evaluating LLMs on SQL Code Generation\n",
        "\n",
        "Evaluating Text-to-SQL might seem like an easy tasks to evaluate as all we need in order to generate our Golden Dataset would be a list of natural language queries and their respective SQL query but there are some intricacies with it that will prove to be challenging.\n",
        "\n",
        "We will need some way to check for **equivalence** to the **gold** query, which is where prior mentioned intricacies will start to pop up, but we will circle back to them later on. With that, we can start off with looking for an exact match between the LLM response and the golden query. For our initial metric we will be using a very simple metric, [exact match](https://huggingface.co/spaces/evaluate-metric/exact_match).\n",
        "\n",
        "![Basic Exact Match LLM Evaluator](https://drive.google.com/uc?id=1yIygL2zNC4JZ02dMBtC5HyNa0_ujIOOv)\n",
        "\n",
        "With a Golden Dataset which we can hand generate using our domain expertise, we can build up an evaluation dataset to run on every iteration to improve our application. Using the exact match metric we can derive an accuracy score of our LLM-application that we can keep track of and use in order to compare the performance across different iterations overtime.\n",
        "\n",
        "Here is an example of what our dataset can look like:\n",
        "\n",
        "![Example Golden Dataset](https://drive.google.com/uc?id=12D8C2juErtOzvKVLaKL_njxlceFJcbTu)\n",
        "\n",
        "**NOTE:** If you noticed, there is a **complexity** column on the SQL code used in the example which we can try incorporating as an optional tasks in order to improve the quality of our results across different fine-grained categories which is paramount in ensuring our LLM SQL Code Generator is more robust. Ideally we should have an even distribution across all these additional categories that we add on, where we can then calculate metrics such as **Accuracy@HardComplexity** and so on.\n",
        "\n",
        "We will provide you with a basic evaluation dataset to start off with, which we can further improve on using the process laid out below using [deepeval](https://github.com/confident-ai/deepeval):\n",
        "\n",
        "![Evaluation Process](https://drive.google.com/uc?id=1KC_gMvU3PeflEm0N1Co8_YnT46lCvMLU)\n",
        "<!-- 1. Load a Golden Test set for the students in a JSONL file with 20-30 examples\n",
        "2. TODO: Create a custom deep-eval evaluator for SQL code where we check if the output of the SQL code exactly matches the output from the LLM.\n",
        "3. TODO: Run evaluation on the golden test set.\n",
        "4. TODO: Try different prompts and see if they improve the evaluation metric\n",
        "5. Checking for an exact match is not the best way to go about it as the same query could have multiple ways to do it. Let's try using G-Eval where we use another more powerful LLM.\n",
        "6. TODO: Evaluate using G-Eval\n",
        "7. Talk about how in a Production system we should be running evaluation just like how we run automated tests, via CI/CD pipelines. -->"
      ],
      "metadata": {
        "id": "pWz1btwOM7zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASIC_GROUND_TRUTH_DATASET = [\n",
        "  {\n",
        "    \"Query\": \"Show the total number of employees.\",\n",
        "    \"Ground Truth\": \"SELECT COUNT(*) AS total_employees FROM employees;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Find the highest salary in the Finance department.\",\n",
        "    \"Ground Truth\": \"SELECT MAX(salary) FROM employees WHERE department = 'Finance';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Get the average age of all managers.\",\n",
        "    \"Ground Truth\": \"SELECT AVG(age) FROM employees WHERE position = 'Manager';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List the names and emails of staff in the IT department.\",\n",
        "    \"Ground Truth\": \"SELECT name, email FROM employees WHERE department = 'IT';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"What are the titles of the top 5 selling books?\",\n",
        "    \"Ground Truth\": \"SELECT title FROM books ORDER BY sales DESC LIMIT 5;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Which product has the least quantity in stock?\",\n",
        "    \"Ground Truth\": \"SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Display the second highest salary in the organization.\",\n",
        "    \"Ground Truth\": \"SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List employees who joined after 2015 and work in the Sales department.\",\n",
        "    \"Ground Truth\": \"SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Find the average order value for each customer.\",\n",
        "    \"Ground Truth\": \"SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Show departments that have more than 10 employees.\",\n",
        "    \"Ground Truth\": \"SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List all products that have never been ordered.\",\n",
        "    \"Ground Truth\": \"SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Which customers have spent more than $1000 in total?\",\n",
        "    \"Ground Truth\": \"SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Show the total number of orders placed each day last week.\",\n",
        "    \"Ground Truth\": \"SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List the names of employees who do not manage anyone.\",\n",
        "    \"Ground Truth\": \"SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Display the department that has the highest average employee salary.\",\n",
        "    \"Ground Truth\": \"SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "dKEiy2NmToCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(BASIC_GROUND_TRUTH_DATASET)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JSrkIWZETx2W",
        "outputId": "91593de8-bfc3-47d8-d38a-deea9db58ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Query  \\\n",
              "0                Show the total number of employees.   \n",
              "1  Find the highest salary in the Finance departm...   \n",
              "2               Get the average age of all managers.   \n",
              "3  List the names and emails of staff in the IT d...   \n",
              "4    What are the titles of the top 5 selling books?   \n",
              "\n",
              "                                        Ground Truth  \n",
              "0  SELECT COUNT(*) AS total_employees FROM employ...  \n",
              "1  SELECT MAX(salary) FROM employees WHERE depart...  \n",
              "2  SELECT AVG(age) FROM employees WHERE position ...  \n",
              "3  SELECT name, email FROM employees WHERE depart...  \n",
              "4  SELECT title FROM books ORDER BY sales DESC LI...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-89914957-9b75-48c9-a667-da670d0bfd70\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>Ground Truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Show the total number of employees.</td>\n",
              "      <td>SELECT COUNT(*) AS total_employees FROM employ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Find the highest salary in the Finance departm...</td>\n",
              "      <td>SELECT MAX(salary) FROM employees WHERE depart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Get the average age of all managers.</td>\n",
              "      <td>SELECT AVG(age) FROM employees WHERE position ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>List the names and emails of staff in the IT d...</td>\n",
              "      <td>SELECT name, email FROM employees WHERE depart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the titles of the top 5 selling books?</td>\n",
              "      <td>SELECT title FROM books ORDER BY sales DESC LI...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89914957-9b75-48c9-a667-da670d0bfd70')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-89914957-9b75-48c9-a667-da670d0bfd70 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-89914957-9b75-48c9-a667-da670d0bfd70');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bfe91f40-cce5-4161-8998-963256e28f54\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bfe91f40-cce5-4161-8998-963256e28f54')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bfe91f40-cce5-4161-8998-963256e28f54 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Show departments that have more than 10 employees.\",\n          \"Which customers have spent more than $1000 in total?\",\n          \"Show the total number of employees.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ground Truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\",\n          \"SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\",\n          \"SELECT COUNT(*) AS total_employees FROM employees;\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, lets see how our LLM and Prompt are able to perform on the entire dataset by running it through and evaluating its performance. This is known as performing a LLM Task Evaluation which is different than LLM Model Evals. Task evaluation cares about the performance of the LLM on a specific task whereas Model evals care about the general performance of the LLM across a variety of tasks, benchmarks such as the [HellaSwag](https://rowanzellers.com/hellaswag/) dataset is an example of a Model Evals and Text2SQL accuracy like we are doing in this project is an example of task evaluation.\n",
        "\n",
        "We will start off with traditional NLP metrics that compare the output with the ground truth can derive a score based on the exact/similarity of the two. These metrics are simple to use and serve as a good place to start off when evaluating LLMs.\n",
        "\n",
        "Firstly, let's start off with the exact match metric which very simply just compares the output with the ground truth and checks whether its an exact match or not, deepeval implements these metrics within the [Scorer module](https://github.com/confident-ai/deepeval/blob/4b3ceed20993232331550798fe0a8f1bf2605594/deepeval/scorer/scorer.py#L7).\n",
        "\n",
        "deepeval makes it easy to create our own metrics by inheriting the BaseMetric class, you can read more about creating custom metrics [here](https://docs.confident-ai.com/docs/metrics-custom)."
      ],
      "metadata": {
        "id": "qBOyfpn54-1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import BaseMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.scorer import Scorer\n",
        "\n",
        "\n",
        "class ExactMatchMetric(BaseMetric):\n",
        "    def __init__(self, threshold: float = 0.0, async_mode: bool = True):\n",
        "        self.threshold = threshold\n",
        "        self.async_mode = async_mode\n",
        "\n",
        "    def _base_measure(self, test_case: LLMTestCase):\n",
        "        self.success = Scorer.exact_match_score(\n",
        "            test_case.actual_output, test_case.expected_output\n",
        "        )\n",
        "        if self.success:\n",
        "            self.score = 1\n",
        "        else:\n",
        "            self.score = 0\n",
        "        return self.score\n",
        "\n",
        "    def measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    async def a_measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    def is_successful(self):\n",
        "        return bool(self.success)\n",
        "\n",
        "    @property\n",
        "    def __name__(self):\n",
        "        return \"ExactMatch\""
      ],
      "metadata": {
        "id": "PPOcKnpcUlD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create our first test case!\n",
        "\n",
        "test_case_input = df.head(1)\n",
        "test_case_query = test_case_input['Query'].squeeze()\n",
        "test_case_expected_output = test_case_input['Ground Truth'].squeeze()"
      ],
      "metadata": {
        "id": "5apnNHGFYvno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Here is our query: {test_case_query}\")\n",
        "print(f\"Here is our ground truth: {test_case_expected_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrLlDUKMb7oq",
        "outputId": "ef46889e-e484-4dc7-b83c-70a2da5f1865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is our query: Show the total number of employees.\n",
            "Here is our ground truth: SELECT COUNT(*) AS total_employees FROM employees;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"\n",
        "Generate a valid SQL query for the following natural language instruction:\n",
        "\n",
        "${query}\n",
        "\n",
        "Only generate SQL code and nothing else.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ZERO_SHOT_PROMPT_TEMPLATE.format(\n",
        "    query=test_case_query\n",
        ")\n",
        "\n",
        "result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "\n",
        "test_case_actual_output = result.generations[0][0].text"
      ],
      "metadata": {
        "id": "7twvbNfOaMaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Here is what our model generated: {test_case_actual_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue1bMG-ubEpt",
        "outputId": "e1a17b63-1a29-4610-bb2d-844734bd9cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is what our model generated: ```sql\n",
            "SELECT COUNT(*) AS total_employees FROM employees;\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "  input=test_case_input,\n",
        "  actual_output=\"SELECT COUNT(*) AS total_employees FROM employees;\",\n",
        "  expected_output=test_case_expected_output\n",
        ")"
      ],
      "metadata": {
        "id": "9FQdZAxXZrQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exact_match_metric = ExactMatchMetric()\n",
        "exact_match_metric.measure(test_case)"
      ],
      "metadata": {
        "id": "9oQx25b7ctYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test case: {exact_match_metric.is_successful()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM2s34m-dBKW",
        "outputId": "43f6b3a1-cbac-4e53-e529-cc9d773fb215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test case: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** If your prompt still fails and produces a different SQL code such as *\"SELECT COUNT(*) AS total_employees FROM employees;\"* feel free to update the ground truth to get it to pass in the meantime. This can happen as code correctness is not a simple task due to the fact that different implementations can still solve the query which is something we invite you to address via the optional tasks for this week.\n",
        "\n",
        "\n",
        "Great job! We are able to see that our LLM and Prompt are able to answer the test case correctly, but that is just a basic example that we have. Next let us run it on the entire dataset that we have created.\n",
        "\n",
        "## TODO: Create a deepeval dataset and evalute our LLM and Prompt on it\n",
        "\n",
        "There is a slight difference between what we have been learning about Golden Datasets in general and how Golden Datasets work within deepeval. When we talk about a Golden Dataset as per the [course materials here](https://uplimit.com/course/llms-in-production/v2/module/evaluating-llms#corise_clrnjsvpd001m2e6iogarwdpn) we mention that its similar to a held-out test set meaning that we have data to predict on alongside their ground truth labels in order to derive evaluation metrics.\n",
        "\n",
        "In deepeval, we will need to create an Evaluation Dataset instead where an Evaluation Dataset is built from LLMTestCase's and/or Golden's. Remember that the `actual_output` comes from running our LLM against the `input` as we did above!\n",
        "```python\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "first_test_case = LLMTestCase(input=\"...\", actual_output=\"...\")\n",
        "second_test_case = LLMTestCase(input=\"...\", actual_output=\"...\")\n",
        "\n",
        "test_cases = [first_test_case, second_test_case]\n",
        "dataset = EvaluationDataset(test_cases=test_cases)\n",
        "```\n",
        "The definition of Golden's within deepeval is documented [here](https://docs.confident-ai.com/docs/confident-ai-manage-datasets#what-is-a-golden):\n",
        "```\n",
        "A \"Golden\" is what makes up an evaluation dataset and is very similar to a test case in deepeval, but they:\n",
        "\n",
        "    - do not require an actual_output, so whilst test cases are always ready for evaluation, a golden isn't.\n",
        "    - only exists within an EvaluationDataset(), while test cases can be defined anywhere.\n",
        "    - contains an extra additional_metadata field, which is a dictionary you can define on Confident. Allows you to do some extra preprocessing on your dataset (eg., generating a custom LLM actual_output based on some variables in additional_metadata) before evaluation.\n",
        "\n",
        "We introduced the concept of goldens because it allows you to create evaluation datasets on Confident without needing pre-computed actual_outputs. This is especially helpful if you are looking to generate responses from your LLM application at evaluation time.\n",
        "```\n",
        "With our EvaluationDataset created we can then run our LLM through it in order to derive the final score. For example, the following does so without using Pytest although its recommended that you treat this process as a CI/CD process that runs alongside your Pytest suite for your actual application.\n",
        "\n",
        "```python\n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "dataset = EvaluationDataset(test_cases=[...])\n",
        "hallucination_metric = HallucinationMetric(threshold=0.3)\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
        "\n",
        "dataset.evaluate([hallucination_metric, answer_relevancy_metric])\n",
        "\n",
        "# You can also call the evaluate() function directly\n",
        "evaluate(dataset, [hallucination_metric, answer_relevancy_metric])\n",
        "```\n",
        "An example output of the evaluation would look like this:\n",
        "```\n",
        "======================================================================\n",
        "\n",
        "Metrics Summary\n",
        "\n",
        "  - ✅ Answer Relevancy (score: 1, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 1.00 because the response perfectly addresses the question about operating hours without any irrelevant information. Great job!, error: None)\n",
        "  - ✅ Bias (score: 0, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 0.00 because the output maintains neutrality and objectivity, as evidenced by the absence of any cited biased phrases or issues., error: None)\n",
        "\n",
        "For test case:\n",
        "\n",
        "  - input: What are your operating hours?\n",
        "  - actual output: ...\n",
        "  - expected output: None\n",
        "  - context: ['Our company operates from 10 AM to 6 PM, Monday to Friday.', 'We are closed on weekends and public holidays.', 'Our customer service is available 24/7.']\n",
        "  - retrieval context: None\n",
        "\n",
        "======================================================================\n",
        "\n",
        "Metrics Summary\n",
        "\n",
        "  - ✅ Answer Relevancy (score: 1, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 1.00 because the response accurately addresses the question about free shipping without any irrelevant information. Great job on maintaining focus!, error: None)\n",
        "  - ✅ Bias (score: 0, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 0.00 because the actual output perfectly demonstrates neutrality and objectivity, without any indication of bias., error: None)\n",
        "\n",
        "For test case:\n",
        "\n",
        "  - input: Do you offer free shipping?\n",
        "  - actual output: ...\n",
        "  - expected output: Yes, we offer free shipping on orders over $50.\n",
        "  - context: None\n",
        "  - retrieval context: None\n",
        "\n",
        "======================================================================\n",
        "```\n",
        "One thing to note is that the metrics are run on a single (query, ground_truth) instead across the entire dataset meaning that we lack the ability to have a singular number to compare across different evaluation runs like traditional machine learning where you would for example have a accuracy score for the test set.\n",
        "\n",
        "Let us introduce the concept of pass rates which are similar to accuracy across the entire dataset but its just meant to show how many instances for a metric in the dataset passed a evaluation metric check.\n",
        "\n",
        "With this you could enforce a strict pass rate on deployments much like how we can enforce Pytest code coverage to be off a certain %.\n",
        "\n",
        "deepeval supports this via the `aggregate_metric_pass_rates()` on the results of the evaluation give us pass rates across each metric that we use:\n",
        "\n",
        "```python\n",
        "from deepeval.evaluate import aggregate_metric_pass_rates\n",
        "\n",
        "evaluation_results = evaluate(dataset, [hallucination_metric, answer_relevancy_metric])\n",
        "aggregate_metric_pass_rates(evaluation_results.test_results)\n",
        "```\n",
        "\n",
        "This should output the following where we get the pass rates across each metric and a mapping storing the pass rates across each metric which we can use to store and compare across different evaluation runs:\n",
        "```\n",
        "======================================================================\n",
        "\n",
        "Aggregate Metric Pass Rates\n",
        "\n",
        "AnswerRelevancyMetric: 100.00% pass rate\n",
        "BiasMetric: 100.00% pass rate\n",
        "\n",
        "======================================================================\n",
        "\n",
        "{'AnswerRelevancyMetric': 1.0, 'BiasMetric': 1.0}\n",
        "```\n",
        "\n",
        "Now with this you can apply it to our dataset:\n",
        "1. Create an [EvaluationDataset](https://docs.confident-ai.com/docs/evaluation-datasets#create-an-evaluation-dataset) using deepeval.\n",
        "2. Run the evaluation on the Golden Dataset without Pytest, refer to [this](https://docs.confident-ai.com/docs/evaluation-introduction#evaluating-without-pytest)."
      ],
      "metadata": {
        "id": "n-A23MUVfNhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# START YOUR CODE HERE\n",
        "######################\n",
        "\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "context = [\"Generate a valid SQL query for the following natural language instruction.\", \"Only generate SQL code and nothing else.\"]\n",
        "first_test_case = LLMTestCase(input=\"Show the total number of employees\", actual_output=\"SELECT COUNT(*) AS total_employees FROM employees;\", context=context)\n",
        "second_test_case = LLMTestCase(input=\"Find the highest salary in the Finance department.\", actual_output=\"SELECT MAX(salary) FROM employees WHERE department = 'Finance';\", context=context)\n",
        "\n",
        "test_cases = [first_test_case, second_test_case]\n",
        "\n",
        "dataset = EvaluationDataset(test_cases=test_cases)\n",
        "\n",
        "hallucination_metric = HallucinationMetric(threshold=0.3)\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
        "\n",
        "# You can also call the evaluate() function directly\n",
        "evaluate(dataset, [hallucination_metric, answer_relevancy_metric])\n",
        "\n"
      ],
      "metadata": {
        "id": "RJ_wARozNj82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79bed210-cef7-4dc6-874e-7d4dd7a5be45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 2 test case(s) in parallel: |██████████|100% (2/2) [Time Taken: 00:06,  3.27s/test case]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because all elements of the actual output align perfectly with the given context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the input question without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the highest salary in the Finance department.\n",
            "  - actual output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - expected output: None\n",
            "  - context: ['Generate a valid SQL query for the following natural language instruction.', 'Only generate SQL code and nothing else.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output perfectly aligns with the context, generating a valid SQL query as instructed, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of employees\n",
            "  - actual output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - expected output: None\n",
            "  - context: ['Generate a valid SQL query for the following natural language instruction.', 'Only generate SQL code and nothing else.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Hallucination: 100.00% pass rate\n",
            "Answer Relevancy: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationResult(test_results=[TestResult(success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because all elements of the actual output align perfectly with the given context, with no contradictions present.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0028874999999999994, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The actual output is a valid SQL query, which agrees with the context of generating a valid SQL query for the given instruction.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The actual output only contains SQL code, which agrees with the context that specifies to generate only SQL code and nothing else.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response perfectly addresses the input question without any irrelevant information. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0027175, verbose_logs='Statements:\\n[\\n    \"SELECT MAX(salary) FROM employees WHERE department = \\'Finance\\';\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Find the highest salary in the Finance department.', actual_output=\"SELECT MAX(salary) FROM employees WHERE department = 'Finance';\", expected_output=None, context=['Generate a valid SQL query for the following natural language instruction.', 'Only generate SQL code and nothing else.'], retrieval_context=None), TestResult(success=True, metrics_data=[MetricData(name='Hallucination', threshold=0.3, success=True, score=0.0, reason='The score is 0.00 because the actual output perfectly aligns with the context, generating a valid SQL query as instructed, with no contradictions present.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.00292, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The actual output is a valid SQL query, which agrees with the context of generating a valid SQL query for a natural language instruction.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The actual output only contains SQL code, which agrees with the context that specifies only generating SQL code and nothing else.\"\\n    }\\n]'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the output perfectly addresses the input without any irrelevant information. Great job!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0026525, verbose_logs='Statements:\\n[\\n    \"SELECT COUNT(*) AS total_employees FROM employees;\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Show the total number of employees', actual_output='SELECT COUNT(*) AS total_employees FROM employees;', expected_output=None, context=['Generate a valid SQL query for the following natural language instruction.', 'Only generate SQL code and nothing else.'], retrieval_context=None)], confident_link=None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good job evaluating the LLM and Prompt on the entire dataset that we have created! If you notice it isn't that good at this task still. What do you think we can do to improve the evaluation performance? What issues do you see with the way we perform evaluations at this point of time?\n",
        "- TODO: ADD YOUR THOUGHTS HERE"
      ],
      "metadata": {
        "id": "7GMJSp2C8jVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Validating LLM Outputs using Guardrails\n",
        "\n",
        "When building our LLM Application one difficult thing is the fact that LLM's are non-deterministic in their outputs. We are able to control different parameters within our LLM to mitigate this to a certain extent such as:\n",
        "1. temperature / top_p\n",
        "2. seed\n",
        "\n",
        "You can read more about these parameters from OpenAI's documentation [here](https://platform.openai.com/docs/api-reference/chat/create).\n",
        "\n",
        "But ensuring consistent outputs is still a problem. *This is where having guardrails comes into play.*\n",
        "\n",
        "> Guardrails = safety controls for LLMs\n",
        "\n",
        "Just like guardrails help prevent cars from going off the road, they can also help out with making sure LLM outputs are in line with set expectations.\n",
        "\n",
        "![Road guardrails](https://drive.google.com/uc?id=1KZCfJs9Sf6ExqpphnTa4G-AaisxFBRWG)\n",
        "\n",
        "Guardrails can take many different forms which differ in complexity:\n",
        "1. **Security Guardrails**, ensuring there isn't any PII data in the response\n",
        "2. **Compliance Guardrails**, ensuring that competitors are not talked about\n",
        "3. **Safety Guardrails**, ensuring that there are not toxic responses\n",
        "4. **Structural Guardrails**, ensuring that the response matches a specific JSON schema\n",
        "\n",
        "\n",
        "A simple Guardrail for this situation could be to ensure that there are **no default error results incorporated within the product description**.\n",
        "\n",
        "In our application we will also be communicating via REST APIs which leverage JSON to send and receive data between different applications.\n",
        "\n",
        "**Frontend JSON sent to the Backend:**\n",
        "```json\n",
        "{\n",
        "    \"prompt\": \"Write a SQL query to find the names and email addresses of all users in the 'users' table who joined after January 1, 2020.\",\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 100\n",
        "}\n",
        "```\n",
        "**Backend Response JSON sent back to the Frontend:**\n",
        "```json\n",
        "{\n",
        "  \"id\": \"cmpl-XYZ456\",\n",
        "  \"object\": \"text_completion\",\n",
        "  \"created\": 1616517999,\n",
        "  \"model\": \"gpt-3.5-turbo\",\n",
        "  \"prediction\": {\n",
        "    \"text\": \"SELECT name, email FROM users WHERE join_date > '2020-01-01';\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "The response from OpenAI will be used to populate the `prediction` field within the main Backend Response where we will have an object that contains a `text` field which will then be used to contain the actual output from OpenAI.\n",
        "\n",
        "In this scenario we would initially look for a **Structural Guardrail** that ensures that the text response from the LLM:\n",
        "1. *is a string*\n",
        "2. *is not empty*\n",
        "\n",
        "A great library for this would be [Guardrails.ai](https://www.guardrailsai.com/) which allows us to leverage [Pydantic](https://docs.pydantic.dev/latest/) in order to easily define validators for our LLM responses.\n"
      ],
      "metadata": {
        "id": "rPmPTQssNDUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!guardrails configure --token $GUARDRAILS_TOKEN"
      ],
      "metadata": {
        "id": "ZSEG6jxaaJFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5487851d-a5d2-412e-9218-feda71fa788d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Enable anonymous metrics reporting? [Y/n]: n\n",
            "Do you wish to use remote inferencing? [Y/n]: y\n",
            "\n",
            "            Login successful.\n",
            "\n",
            "            Get started by installing our RegexMatch validator:\n",
            "            https://hub.guardrailsai.com/validator/guardrails_ai/regex_match\n",
            "\n",
            "            You can install it by running:\n",
            "            guardrails hub install hub://guardrails/regex_match\n",
            "\n",
            "            Find more validators at https://hub.guardrailsai.com\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class LLMResponse(BaseModel):\n",
        "  generated_sql: str = Field(description=\"Generated SQL from LLM\")"
      ],
      "metadata": {
        "id": "JUf5Y8tizDdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from rich import print\n",
        "from guardrails import Guard\n",
        "\n",
        "# TODO: Add in your prompt here from before along with the query\n",
        "# NOTE: gr.complete_json_suffix_v2 comes from guardrail where it will inject\n",
        "#       additional tokens related to enforcing the Pydantic Model, to the main prompt.\n",
        "ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE = \"\"\"\n",
        "Generate a valid SQL query for the following natural language instruction:\n",
        "${query}\n",
        "Only generate SQL code and nothing else.\n",
        "\n",
        "${gr.complete_json_suffix_v3}\n",
        "\"\"\"\n",
        "\n",
        "guard = Guard.from_pydantic(output_class=LLMResponse, prompt=ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE)\n",
        "\n",
        "raw_llm_output, validated_output, *rest = guard(\n",
        "    llm_api=openai.chat.completions.create,\n",
        "    model=MODEL_NAME,\n",
        "    prompt_params={\n",
        "        \"query\": SAMPLE_QUERY\n",
        "    },\n",
        ")\n"
      ],
      "metadata": {
        "id": "6E10suedyZwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(guard.history.last.prompt)"
      ],
      "metadata": {
        "id": "PRoWerMjB1um",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "08175f2f-6d75-4544-d248-cd686215f276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Generate a valid SQL query for the following natural language instruction:\n",
              "$\u001b[1m{\u001b[0mquery\u001b[1m}\u001b[0m\n",
              "Only generate SQL code and nothing else.\n",
              "\n",
              "$\u001b[1m{\u001b[0mgr.complete_json_suffix_v3\u001b[1m}\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Generate a valid SQL query for the following natural language instruction:\n",
              "$<span style=\"font-weight: bold\">{</span>query<span style=\"font-weight: bold\">}</span>\n",
              "Only generate SQL code and nothing else.\n",
              "\n",
              "$<span style=\"font-weight: bold\">{</span>gr.complete_json_suffix_v3<span style=\"font-weight: bold\">}</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_llm_output)"
      ],
      "metadata": {
        "id": "utarYL7rTmcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f9d933c-e725-400e-f1fa-0b6c2e435e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m\"generated_sql\"\u001b[0m:\u001b[32m\"SELECT name FROM employees ORDER BY salary DESC LIMIT 1;\"\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"generated_sql\"</span>:<span style=\"color: #008000; text-decoration-color: #008000\">\"SELECT name FROM employees ORDER BY salary DESC LIMIT 1;\"</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(validated_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "otmh76saBwHz",
        "outputId": "be2ee666-8afd-477a-9080-2c1f4c980f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m'generated_sql'\u001b[0m: \u001b[32m'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_sql'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can view the history of our prompts in a nice tree like format\n",
        "print(guard.history.last.tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "NMhpA6jtDlLJ",
        "outputId": "554df7a6-bb3d-41e3-cf55-1a78b4eebd73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Logs\n",
              "└── ╭────────────────────────────────────────────────── Step 0 ───────────────────────────────────────────────────╮\n",
              "    │ \u001b[48;2;240;248;255m╭─\u001b[0m\u001b[48;2;240;248;255m───────────────────────────────────────────────\u001b[0m Prompt \u001b[48;2;240;248;255m────────────────────────────────────────────────\u001b[0m\u001b[48;2;240;248;255m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mGenerate a valid SQL query for the following natural language instruction:\u001b[0m\u001b[48;2;240;248;255m                             \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mSelect the name of the employee who has the highest salary\u001b[0m\u001b[48;2;240;248;255m                                             \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mOnly generate SQL code and nothing else.\u001b[0m\u001b[48;2;240;248;255m                                                               \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mGiven below is JSON Schema that describes the information to extract from this document and the tags to\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mextract it into.\u001b[0m\u001b[48;2;240;248;255m                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m{\"properties\": {\"generated_sql\": {\"description\": \"Generated SQL from LLM\", \"title\": \"Generated Sql\", \u001b[0m\u001b[48;2;240;248;255m  \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m\"type\": \"string\"}}, \"required\": [\"generated_sql\"], \"type\": \"object\", \"title\": \"LLMResponse\"}\u001b[0m\u001b[48;2;240;248;255m           \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mONLY return a valid JSON object (no other text is necessary). The JSON MUST conform to the JSON Schema,\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mincluding any types and format requests e.g. requests for lists, objects and specific types. Be correct\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mand concise. If you are unsure anywhere, try your best guess.\u001b[0m\u001b[48;2;240;248;255m                                          \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mHere are examples of simple (JSON Schema, JSON) pairs that show the expected behavior:\u001b[0m\u001b[48;2;240;248;255m                 \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- `{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}` => \u001b[0m\u001b[48;2;240;248;255m       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{'foo': 'example one'}`\u001b[0m\u001b[48;2;240;248;255m                                                                               \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- \u001b[0m\u001b[48;2;240;248;255m                                                                                                     \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}}}\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m}` => `{\"bar\": ['STRING ONE', 'STRING TWO']}`\u001b[0m\u001b[48;2;240;248;255m                                                          \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- \u001b[0m\u001b[48;2;240;248;255m                                                                                                     \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{\"type\":\"object\",\"properties\":{\"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"c\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mapitalize two-words\"},\"index\":{\"type\":\"integer\",\"format\":\"1-indexed\"}}}}}` => `{'baz': {'foo': 'Some \u001b[0m\u001b[48;2;240;248;255m  \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mString', 'index': 1}}`\u001b[0m\u001b[48;2;240;248;255m                                                                                 \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- \u001b[0m\u001b[48;2;240;248;255m                                                                                                     \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}},\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m\"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}}}` => \u001b[0m\u001b[48;2;240;248;255m  \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{'bar': ['STRING ONE', 'STRING TWO'], 'baz': {'foo': 'example one'}}`\u001b[0m\u001b[48;2;240;248;255m                                 \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m╭─\u001b[0m\u001b[48;2;255;240;242m────────────────────────────────────────────\u001b[0m Instructions \u001b[48;2;255;240;242m─────────────────────────────────────────────\u001b[0m\u001b[48;2;255;240;242m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m│\u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242mYou are a helpful assistant, able to express yourself purely through JSON, strictly and precisely \u001b[0m\u001b[48;2;255;240;242m     \u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242m│\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m│\u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242madhering to the provided JSON schema.\u001b[0m\u001b[48;2;255;240;242m                                                                  \u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242m│\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m╭─\u001b[0m\u001b[48;2;231;223;235m───────────────────────────────────────────\u001b[0m Message History \u001b[48;2;231;223;235m───────────────────────────────────────────\u001b[0m\u001b[48;2;231;223;235m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m│\u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235mNo message history.\u001b[0m\u001b[48;2;231;223;235m                                                                                    \u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m│\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m╭─\u001b[0m\u001b[48;2;245;245;220m───────────────────────────────────────────\u001b[0m Raw LLM Output \u001b[48;2;245;245;220m────────────────────────────────────────────\u001b[0m\u001b[48;2;245;245;220m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m{\"generated_sql\":\"SELECT name FROM employees ORDER BY salary DESC LIMIT 1;\"}\u001b[0m\u001b[48;2;245;245;220m                           \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m╭─\u001b[0m\u001b[48;2;240;255;240m──────────────────────────────────────────\u001b[0m Validated Output \u001b[48;2;240;255;240m───────────────────────────────────────────\u001b[0m\u001b[48;2;240;255;240m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m│\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m{'generated_sql': 'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'}\u001b[0m\u001b[48;2;240;255;240m                          \u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Logs\n",
              "└── ╭────────────────────────────────────────────────── Step 0 ───────────────────────────────────────────────────╮\n",
              "    │ <span style=\"background-color: #f0f8ff\">╭────────────────────────────────────────────────</span> Prompt <span style=\"background-color: #f0f8ff\">─────────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Generate a valid SQL query for the following natural language instruction:                              │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Select the name of the employee who has the highest salary                                              │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Only generate SQL code and nothing else.                                                                │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Given below is JSON Schema that describes the information to extract from this document and the tags to │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ extract it into.                                                                                        │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ {\"properties\": {\"generated_sql\": {\"description\": \"Generated SQL from LLM\", \"title\": \"Generated Sql\",    │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ \"type\": \"string\"}}, \"required\": [\"generated_sql\"], \"type\": \"object\", \"title\": \"LLMResponse\"}            │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ ONLY return a valid JSON object (no other text is necessary). The JSON MUST conform to the JSON Schema, │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ including any types and format requests e.g. requests for lists, objects and specific types. Be correct │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ and concise. If you are unsure anywhere, try your best guess.                                           │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Here are examples of simple (JSON Schema, JSON) pairs that show the expected behavior:                  │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ - `{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}` =&gt;         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{'foo': 'example one'}`                                                                                │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ -                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}}} │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ }` =&gt; `{\"bar\": ['STRING ONE', 'STRING TWO']}`                                                           │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ -                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{\"type\":\"object\",\"properties\":{\"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"c │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ apitalize two-words\"},\"index\":{\"type\":\"integer\",\"format\":\"1-indexed\"}}}}}` =&gt; `{'baz': {'foo': 'Some    │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ String', 'index': 1}}`                                                                                  │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ -                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}}, │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ \"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}}}` =&gt;    │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{'bar': ['STRING ONE', 'STRING TWO'], 'baz': {'foo': 'example one'}}`                                  │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">╭─────────────────────────────────────────────</span> Instructions <span style=\"background-color: #fff0f2\">──────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">│ You are a helpful assistant, able to express yourself purely through JSON, strictly and precisely       │</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">│ adhering to the provided JSON schema.                                                                   │</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">╭────────────────────────────────────────────</span> Message History <span style=\"background-color: #e7dfeb\">────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">│ No message history.                                                                                     │</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">╭────────────────────────────────────────────</span> Raw LLM Output <span style=\"background-color: #f5f5dc\">─────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">│ {\"generated_sql\":\"SELECT name FROM employees ORDER BY salary DESC LIMIT 1;\"}                            │</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">╭───────────────────────────────────────────</span> Validated Output <span style=\"background-color: #f0fff0\">────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">│ {'generated_sql': 'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'}                           │</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Adding More Guardrails\n",
        "We have created a very basic Guardrail that leverages Pydantic Model's to ensure that our response fits a specifc format. Now let us try more advanced guardrails:\n",
        "- Try creating **Bug Free SQL Code** leveraging the following [example](https://www.guardrailsai.com/docs/examples/syntax_error_free_sql).\n",
        "- Feel free to add in more [guardrails via the Guardrails.ai Hub](https://hub.guardrailsai.com/) where you see fit for our use case. For example, we could ensure that there isn't any toxic language being returned from out LLM via using the [ToxicLanguage Validator](https://www.guardrailsai.com/docs/examples/toxic_language).\n",
        "- How should we handle validation failures? One way would be to leverage reasking, where we reask the LLM to fix the output from a validation check failure. Within Guardrails.ai there are corrective measures we can take to handle validation errors as outlined [here](https://www.guardrailsai.com/docs/how_to_guides/rail#%EF%B8%8F-specifying-corrective-actions).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KtDOZyPK2D4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# START YOUR CODE HERE\n",
        "######################\n",
        "!pip install sqlvalidator -q\n",
        "!guardrails hub install hub://guardrails/valid_sql"
      ],
      "metadata": {
        "id": "XQ-2FIj2NnaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad15a18-6a37-4fe2-f88e-205870a28562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mvalid_sql...\u001b[0m\n",
            "\u001b[2K\u001b[32m[  ==]\u001b[0m Fetching manifest\n",
            "\u001b[2K\u001b[32m[====]\u001b[0m Downloading dependencies  Running command git clone --filter=blob:none --quiet https://github.com/guardrails-ai/valid_sql.git /tmp/pip-req-build-th3dzo3l\n",
            "\u001b[2K\u001b[32m[==  ]\u001b[0m Downloading dependencies\n",
            "\u001b[1A\u001b[2K✅Successfully installed guardrails/valid_sql!\n",
            "\n",
            "\n",
            "\u001b[1mImport validator:\u001b[0m\n",
            "from guardrails.hub import ValidSQL\n",
            "\n",
            "\u001b[1mGet more info:\u001b[0m\n",
            "\u001b[4;94mhttps://hub.guardrailsai.com/validator/guardrails/valid_sql\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from guardrails import Guard\n",
        "from guardrails.hub import ValidSQL\n",
        "\n",
        "# Setup Guard\n",
        "guard = Guard().use(\n",
        "    ValidSQL, on_fail=\"exception\"\n",
        ")\n",
        "response = guard.validate(\"SELECT * FROM EMPLOYEES;\")  # Validator passes\n",
        "\n",
        "try:\n",
        "    response = guard.validate(\"SELEKT ID FROM USERS;\")  # Validator fails\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n"
      ],
      "metadata": {
        "id": "kXHu_FaYymnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job adding in robust guardrails, next we can now focus on making sure our system performs at peak performance. Let us try improving the results of the model using Prompt Engineering techniques making sure that we compare results from the baseline with the new improvements. Feel free to go wild and experiment with all kinds of techniques for this section!\n",
        "\n",
        "## TODO: Evaluation Driven Development(EDD)\n",
        "\n",
        "EDD is similar to [Test Driven Development(TDD)](https://martinfowler.com/bliki/TestDrivenDevelopment.html) except we apply it to building LLM-powered applications, where our test cases in this case are derived from the Golden dataset that we created earlier and the goal here is to iteratively ensure that our LLM application can achieve a **100% pass rate** across all the examples.\n",
        "\n",
        "The main idea here is that if we currently have a 60% pass rate on the exact match metric, we should take an iterative approach towards increasing the pass rate. For example, the LLM could be failing on the following query `Display the department that has the highest average employee salary.`. In this section we will try to tune the prompt or other parts to ensure that the model is able to correctly generate the ground truth for it.\n",
        "\n",
        "NOTE: As we mentioned before some items from the dataset could result in false negatives since multiple implementations are possible for the same query.\n",
        "\n",
        "Here are a couple of things we can try out:\n",
        "- Try out Few-Shot Prompting and [other techniques](https://www.promptingguide.ai/techniques) and see how it impacts our evaluation performance.\n",
        "- Try tuning the parameters used for a model.\n",
        "- Try out different LLM models from OpenAI, Anthropic, etc. if you have API keys for them.\n",
        "\n",
        "You will have to setup an EvaluationDataset per each comparison since each run would have a different `actual_output` used within the LLMTestCase.\n",
        "```python\n",
        "from deepeval.evaluate import aggregate_metric_pass_rates\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "dataset_run_1 = EvaluationDataset(test_cases=[...])\n",
        "dataset_run_2 = EvaluationDataset(test_cases=[...])\n",
        "\n",
        "evaluation_results_run_1 = evaluate(dataset_run_1, [exact_match_metric])\n",
        "pass_rate_run_1 = aggregate_metric_pass_rates(evaluation_results_run_1.test_results)\n",
        "\n",
        "evaluation_results_run_2 = evaluate(dataset_run_2, [exact_match_metric])\n",
        "pass_rate_run_2 = aggregate_metric_pass_rates(evaluation_results_run_2.test_results)\n",
        "\n",
        "assert pass_rate_run_2.get(\"ExactMatch\") > pass_rate_run_1.get(\"ExactMatch\"), \\\n",
        "  \"Run 2 does not perform better than Run 1\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "VgWBsMZ-Ex96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# START YOUR CODE HERE\n",
        "######################\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import LLMResult, HumanMessage, Generation\n",
        "from deepeval.evaluate import aggregate_metric_pass_rates\n",
        "\n",
        "\n",
        "def generate_eval_dataset(context) :\n",
        "  first_test_case = LLMTestCase(input=\"Show the total number of employees\", actual_output=\"SELECT COUNT(*) AS total_employees FROM employees;\", context=context)\n",
        "  second_test_case = LLMTestCase(input=\"Find the highest salary in the Finance department.\", actual_output=\"SELECT MAX(salary) FROM employees WHERE department = 'Finance';\", context=context)\n",
        "\n",
        "  test_cases = [first_test_case, second_test_case]\n",
        "\n",
        "  dataset = EvaluationDataset(test_cases=test_cases)\n",
        "  return dataset\n",
        "\n",
        "dataset_run_1 = generate_eval_dataset(context=[\"Generate a valid SQL query for the following natural language instruction.\", \"Only generate SQL code and nothing else.\"])\n",
        "\n",
        "dataset_run_2 = generate_eval_dataset(context=[\"Only generate SQL code and nothing else.\",\n",
        "                                               \"Generate the SQL query for each of the following natural language questions.\",\n",
        "                                               \"Question: How many employees are there in the company? SQL: SELECT COUNT(*) FROM employees;\",\n",
        "                                               \"Question: List all employees hired after January 1, 2020. SQL: SELECT * FROM employees WHERE hire_date > '2020-01-01'\"])\n",
        "\n",
        "evaluation_results_run_1 = evaluate(dataset_run_1, [exact_match_metric])\n",
        "pass_rate_run_1 = aggregate_metric_pass_rates(evaluation_results_run_1.test_results)\n",
        "\n",
        "evaluation_results_run_2 = evaluate(dataset_run_2, [exact_match_metric])\n",
        "pass_rate_run_2 = aggregate_metric_pass_rates(evaluation_results_run_2.test_results)\n",
        "\n"
      ],
      "metadata": {
        "id": "fBl8t9QZNovv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4df500b6-b04b-4e98-e20e-a96ecdc8bdcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mExactMatch Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">ExactMatch Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 2 test case(s) in parallel: |██████████|100% (2/2) [Time Taken: 00:00, 324.46test case/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of employees\n",
            "  - actual output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - expected output: None\n",
            "  - context: ['Generate a valid SQL query for the following natural language instruction.', 'Only generate SQL code and nothing else.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the highest salary in the Finance department.\n",
            "  - actual output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - expected output: None\n",
            "  - context: ['Generate a valid SQL query for the following natural language instruction.', 'Only generate SQL code and nothing else.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "ExactMatch: 0.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "ExactMatch: 0.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mExactMatch Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">ExactMatch Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 2 test case(s) in parallel: |██████████|100% (2/2) [Time Taken: 00:00, 273.70test case/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of employees\n",
            "  - actual output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - expected output: None\n",
            "  - context: ['Only generate SQL code and nothing else.', 'Generate the SQL query for each of the following natural language questions.', 'Question: How many employees are there in the company? SQL: SELECT COUNT(*) FROM employees;', \"Question: List all employees hired after January 1, 2020. SQL: SELECT * FROM employees WHERE hire_date > '2020-01-01'\"]\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ ExactMatch (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the highest salary in the Finance department.\n",
            "  - actual output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - expected output: None\n",
            "  - context: ['Only generate SQL code and nothing else.', 'Generate the SQL query for each of the following natural language questions.', 'Question: How many employees are there in the company? SQL: SELECT COUNT(*) FROM employees;', \"Question: List all employees hired after January 1, 2020. SQL: SELECT * FROM employees WHERE hire_date > '2020-01-01'\"]\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "ExactMatch: 0.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "ExactMatch: 0.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Evaluating SQL Code Using a Stronger LLM\n",
        "\n",
        "Usually we would prioritise latency during inference using LLMs for most use cases, LLMs such as `gpt3.5-turbo` are much faster than `gpt4` variants while performing worse in general across benchmarks. What if we could also use a bigger and stronger LLM to aid with evaluations as well? There has been a rise of techniques doing just that where a smaller and faster LLM is evaluated by a bigger and stronger LLM. This process helps automate the evaluation part of any LLM-powered application! This is kinda of like having a dream within a dream just like in the movie *Inception*.\n",
        "\n",
        "![Dream within a dream from Inception](https://drive.google.com/uc?id=1EC7rNg_LCLjIJ9SRyQpGVgtplOHZllDm)\n",
        "\n",
        "\n",
        "**NOTE:** LLMs are very powerful and can help automate so many parts of the process but becareful not to be too reliant on them, at the end of the day they aren't perfect and hallucinate a lot. Therefore, its vital that you always keep a human-in-the-loop to verify things as a final seal of approval.\n",
        "\n",
        "\n",
        "### Introducing G-Eval\n",
        "\n",
        "[G-Eval](https://arxiv.org/abs/2303.16634) is a method that utilizes LLMs alongside a Chain-of-Thought (CoT) and form-filling approach to assess the outputs of LLMs. Initially, a task outline and evaluation parameters are provided to an LLM, which is then requested to create a CoT detailing the evaluation steps. For assessing the coherence of news summaries, the prompt, CoT, news article, and summary are merged, and the LLM is instructed to produce a score ranging from 1 to 5. Subsequently, the output token probabilities generated by the LLM are used to standardize this score, with a weighted summation taken as the ultimate outcome.\n",
        "\n",
        "It was observed that using GPT-4 as an evaluator resulted in a high Spearman correlation with human assessments (0.514), surpassing all former methods. In the realm of summarization, it exceeded all prior state-of-the-art (SOTA) evaluators in the SummEval benchmark, which evaluates coherence, consistency, fluency, and relevance.\n",
        "\n",
        "\n",
        "![G-Eval](https://drive.google.com/uc?id=1dq1BAjLY_Es5r-UiD96ZnFlxWTYcNv8g)\n",
        "\n",
        "deepeval already comes out of the box with support for using G-Eval [here](https://docs.confident-ai.com/docs/metrics-llm-evals) where you can very easily just create a new metric and run it against your EvaluationDataset:\n",
        "\n",
        "```python\n",
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCaseParams\n",
        "\n",
        "correctness_metric = GEval(\n",
        "    name=\"Correctness\",\n",
        "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
        "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
        "    evaluation_steps=[\n",
        "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
        "        \"You should also heavily penalize omission of detail\",\n",
        "        \"Vague language, or contradicting OPINIONS, are OK\"\n",
        "    ],\n",
        "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
        ")\n",
        "```\n",
        "Try creating a G-Eval metric for SQL Code Generation and running it through our EvaluationDataset like before.\n",
        "\n",
        "\n",
        "**NOTE:** The API Key provided to you only allows access to `gpt-4o-mini`, we can still use it either way but in the real world you would ideally use a more powerful model for this task. If you were to use `gpt-4o` as your base model then you probably don't have another *stronger* LLM to use therefore this also mimics that kind of constraint."
      ],
      "metadata": {
        "id": "pnh0hFV-HLK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# START YOUR CODE HERE\n",
        "######################\n",
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCaseParams\n",
        "\n",
        "correctness_metric = GEval(\n",
        "    name=\"Correctness\",\n",
        "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
        "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
        "    evaluation_steps=[\n",
        "        \"Check whether the SQL in 'actual output' basically does the same thing as 'expected output'\",\n",
        "        \"If it's only kind of close, that's fine, sort of\"\n",
        "    ],\n",
        "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
        ")\n",
        "evaluation_gval_results = evaluate(dataset_run_1, [correctness_metric])"
      ],
      "metadata": {
        "id": "VgazsrdbRouA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "outputId": "97270eb6-f245-48ae-83f5-67a97ce89dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 2 test case(s) in parallel: |██████████|100% (2/2) [Time Taken: 00:01,  1.50test case/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Correctness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The actual SQL correctly counts the total number of employees from the employees table, aligning perfectly with the input request., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of employees\n",
            "  - actual output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - expected output: None\n",
            "  - context: ['Generate a valid SQL query for the following natural language instruction.', 'Only generate SQL code and nothing else.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Correctness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The SQL query accurately finds the highest salary in the Finance department using MAX(salary) and filtering by department='Finance'., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the highest salary in the Finance department.\n",
            "  - actual output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - expected output: None\n",
            "  - context: ['Generate a valid SQL query for the following natural language instruction.', 'Only generate SQL code and nothing else.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Correctness (GEval): 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job on getting G-Eval to work with our dataset! What do you think about using LLMs to evaluate another LLM vs just using traditional Machine Learning metrics? What are the Pros/Cons of it and where do you think it would work better or worse in?\n",
        "\n",
        "**TODO: Add your thoughts here!**"
      ],
      "metadata": {
        "id": "HC6hxGkHRuEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Extra Credit: Optional Tasks\n",
        "\n",
        "- Try out implementing the Execution Score from the Snowflake team and comparing the results of it to Exact Match Metric and G-Eval, this [blog](https://medium.com/snowflake/inside-snowflake-building-the-most-powerful-sql-llm-in-the-world-1a33b3ee0d37) from the Snowflake Team covers how you can use LLMs to assists with evaluation.\n",
        "- If you use SQL enough you would know that there are multiple ways in which we can answer a particular query especially if you had to update your ground truth earlier. With that you can try out other frameworks such as [sql-eval](https://github.com/defog-ai/sql-eval) which provides mo like we did before but more importanly also includes code execution into context when evaluating LLMs.\n",
        "- Guardrails do not just apply to the output from an LLM, instead we can also apply them to the actual input that a user feeds into the LLM. You can try out **Detecting Toxicity**, **Restricting the input to just be related to SQL code generation**, **Limiting the amount of tokens used**, etc.\n",
        "- Our BASIC_GROUND_TRUTH_DATASET is quite small, see how you can add in more examples using your own domain knowledge or you could even leverage an LLM to do so via synthetic data generation which is supported within deepeval [here](https://docs.confident-ai.com/docs/evaluation-datasets-synthetic-data). Remember not all users will put in queries that can be answered with SQL code. For example, if the user asks `Who will win the next US Presidential Election?` you definitely don't want to answer it at all!\n"
      ],
      "metadata": {
        "id": "UL-XI8PiNU1z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIcvjLeVSEeD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}